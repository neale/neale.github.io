<!DOCTYPE html>
<html>
    <head>
        <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script>
          MathJax.Hub.Config({
              config: ["MMLorHTML.js"],
              extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js"],
              jax: ["input/TeX"],
              tex2jax: {
                  inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                  displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                  processEscapes: false
              },
              TeX: {
                  TagSide: "right",
                  TagIndent: ".8em",
                  MultLineWidth: "85%",
                  equationNumbers: {
                     autoNumber: "AMS",
                  },
                  unicode: {
                     fonts: "STIXGeneral,'Arial Unicode MS'"
                  }
              },
              showProcessingMessages: false
            });
        </script> 
      <meta charset="utf-8">
          <title>Breaking Defensive Distillation with L-BFGS</title>
          <meta name="viewport" content="width=device-width, initial-scale=1">
          <link rel="shortcut icon" href="/assets/images/top_icon.png">
          <!-- <link href="http://fonts.googleapis.com/css?family=Source+Sans+Pro:200,400" rel="stylesheet" type="text/css">-->
          <link href="https://fonts.googleapis.com/css?family=Nunito+Sans:200,400" rel="stylesheet" type="text/css">
          <link href="/assets/css/post.css" rel="stylesheet">
          <link href="/assets/css/syntax.css" rel="stylesheet">

    </head>
    <body>
        <header class="mobile-header">
        <input id="burger" type="checkbox" />

        <label for="burger">
            <span></span>
            <span></span>
            <span></span>
        </label>

        <nav>    
          <ul>
              <li><a href="/blog" class="header-links">Deep Learning</a></li>
              <li><a href="http://github.com/neale" class="header-links">GitHub</a></li>
              <li><a href="https://www.linkedin.com/in/neale-ratzlaff-46879973/" class="header-links">LinkedIn</a></li>
              <li><a href="http://github.com/neale/CPPN" class="header-links">Generative Art</a></li>
          </ul>  
        </nav>
        </header>

        <div class="outer-container">
            <div class="inner-container">
              <header class="site-header">
                <nav id="nav-toggle">
                  <ul class="nav navbar-nav">
                    <li><a href="/" class="header-links">Home</a><li>
                    <li><a href="/blog" class="header-links">Deep Learning</a></li>
                    <li><a href="http://github.com/neale" class="header-links">GitHub</a></li>
                    <li><a href="http://github.com/neale/CPPN" class="header-links">Generative Art</a></li>
                  </ul>
                </nav>
              </header>
       
 
                <!--start of the post title-->
    <section class="single-post-title-main">
        <h2>Breaking Defensive Distillation with L-BFGS</h2>
    </section>
<!--End of the post title-->

<!--Start of the section post-->
    <section class="post">
       <p>
          <p>Defensive distillation was a good idea: the authors basically fuzz the gradients of the network by training a second smaller network on the softmax outputs of the original network.</p>

<p>So that a training sample of the second network has a distribution of <script type="math/tex">\textit{soft labels}</script> assigned as the label, instead of a single hard label where <script type="math/tex">\hat{y} = \textit{argmax}(f(x))</script>. This increases the number of features that need to be changed by an adversarial attack, in order to change the output label. Thatâ€™s a high level overview, I recommend the actual paper <a href="https://arxiv.org/pdf/1511.04508.pdf">here</a></p>

<p>This worked pretty well at first, until it was broken by <a href="https://arxiv.org/pdf/1607.04311.pdf">Carlini and Wagner</a> with an iterative method that targets logits instead of the softmax output. 
Distillation relies heavily on the temperature of the primary network in order to train the distillation net. 
Which controls the distribution of soft labels that each example is assigned. 
Recap: a softmax output creates a <script type="math/tex">[0, 1]</script> normalized distribution from the incoming logits.</p>

<script type="math/tex; mode=display">F(y = \textit{softmax}(Z(\theta, x) / T)</script>

<script type="math/tex; mode=display">\textit{softmax}() = \frac{e^{zj}}{\sum_{k} e^{z_k}}</script>

<p>Where the temperature <script type="math/tex">T</script> controls the shape of the output distribtion. 
When <script type="math/tex">T \rightarrow \infty</script> the output becomes uniform, when <script type="math/tex">T \rightarrow 0</script> the distribution become unimodal.</p>


       </p>

    </section>


        </div><!-- /.inner-container -->
      </div>  <!-- /.outer-container -->
        </footer>

              </body>

</html>
