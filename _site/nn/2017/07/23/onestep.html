<!DOCTYPE html>
<html>
    <head>
        <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script>
          MathJax.Hub.Config({
              config: ["MMLorHTML.js"],
              extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js"],
              jax: ["input/TeX"],
              tex2jax: {
                  inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                  displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                  processEscapes: false
              },
              TeX: {
                  TagSide: "right",
                  TagIndent: ".8em",
                  MultLineWidth: "85%",
                  equationNumbers: {
                     autoNumber: "AMS",
                  },
                  unicode: {
                     fonts: "STIXGeneral,'Arial Unicode MS'"
                  }
              },
              showProcessingMessages: false
            });
        </script> 
      <meta charset="utf-8">
          <title>Removing One-Step Adversarial Perturbations with Bilateral Filtering</title>
          <meta name="viewport" content="width=device-width, initial-scale=1">
          <link rel="shortcut icon" href="/assets/images/top_icon.png">
          <!-- <link href="http://fonts.googleapis.com/css?family=Source+Sans+Pro:200,400" rel="stylesheet" type="text/css">-->
          <link href="https://fonts.googleapis.com/css?family=Nunito+Sans:200,400" rel="stylesheet" type="text/css">
          <link href="/assets/css/post.css" rel="stylesheet">
          <link href="/assets/css/syntax.css" rel="stylesheet">

    </head>
    <body>
        <header class="mobile-header">
        <input id="burger" type="checkbox" />

        <label for="burger">
            <span></span>
            <span></span>
            <span></span>
        </label>

        <nav>    
          <ul>
              <li><a href="/blog" class="header-links">Deep Learning</a></li>
              <li><a href="http://github.com/neale" class="header-links">GitHub</a></li>
              <li><a href="https://www.linkedin.com/in/neale-ratzlaff-46879973/" class="header-links">LinkedIn</a></li>
              <li><a href="http://neale.github.io/genart" class="header-links">Generative Art</a></li>
          </ul>  
        </nav>
        </header>

        <div class="outer-container">
            <div class="inner-container">
              <header class="site-header">
                <nav id="nav-toggle">
                  <ul class="nav navbar-nav">
                    <li><a href="/" class="header-links">Home</a><li>
                    <li><a href="/blog" class="header-links">Deep Learning</a></li>
                    <li><a href="http://github.com/neale" class="header-links">GitHub</a></li>
                    <li><a href="http://neale.github.io/genart" class="header-links">Generative Art</a></li>
                  </ul>
                </nav>
              </header>
       
 
                <!--start of the post title-->
    <section class="single-post-title-main">
        <h2>Removing One-Step Adversarial Perturbations with Bilateral Filtering</h2>
    </section>
<!--End of the post title-->

<!--Start of the section post-->
    <section class="post">
       <p>
          <p>I don’t think its escaped anyone that the state of the art of adversarial attacks completely outstrips the state of the art for defenses. 
All defenses including defensive distillation, logit classification a la. SafetyNet, and other statistical methods are currently broken. 
Mostly, these defenses fall to optimization based adversarial attacks, because the current way we do things in machine learning and computer vision is invariably to train a classifier.</p>

<script type="math/tex; mode=display">\textbf{y} = \mathit{overfit}(\textbf{x})</script>

<p>In the context of adversarial attacks, training regimes invaribly create fully exploitable models. 
The CW attacks are all optimization based attacks, which are very much standard now. Optimization based attacks such as CW, the L-BFGS attack, and the Jacobian Based Saliency Map attack are far more effective at creating adversarial attacks than so called one-step attacks.
Most optimization attacks work by taking some image <script type="math/tex">x</script> with some label <script type="math/tex">y</script>, and repeatly minimizing some objective s.t. some norm to get a similar image <script type="math/tex">x'</script> with a different label <script type="math/tex">y'</script></p>

<p>This is in contrast to one-step methods, which constructs the adversarial perturbation in a single go. 
The common example is the Fast Gradient Sign Method, which adds a perturbation <script type="math/tex">\eta</script> in the direction of the incorrect class.</p>

<script type="math/tex; mode=display">\eta = \textit{sign}(\nabla_x H(x, y'))</script>

<p>Where <script type="math/tex">\textit{H(x, y')}</script> is the cross entropy loss w.r.t an image and an incorrect label.
Adversarial examples can be created quickly using only a single gradient computation.</p>

<p>Compare this to some of the different objective functions of iterative methods, using the same notation as above. <script type="math/tex">Z</script> here refers to the network logits, i.e. the input of the softmax layer,</p>

<h5 id="l-bfgs">L-BFGS</h5>

<script type="math/tex; mode=display">\min c || x - x' ||^{2}_{2} + H(x')</script>

<h5 id="cw-l_2">CW (<script type="math/tex">L_2</script>)</h5>

<script type="math/tex; mode=display">\min ||\eta - x||^2_2 + c f(\eta)</script>

<script type="math/tex; mode=display">\textit{where} \quad f(x') = \max(Z(x')_i - Z(x')_{y'}) \quad 
\textit{such that} \quad i \neq y'</script>

<p>I’ve simplified the above equations; removing the box constraints and the CW change of variables for readability. 
But the point is that we can choose a metric, and can optimize against our network to create effective adversarial examples with minimal distortion.</p>

<div class="post-img">
<p align="center">
<img class="img-responsive img-post" src=" /assets/images/snake-adv-comp.svg" align="middle" width="90%" height="90%" />
</p>
</div>

<p>Given the above difference, the rationale behind one-step attacks seems suspect. Why bother with them at all if optimization based methods are so superior that the magnitude of the (normalized) perturbation is less than 1 across a <script type="math/tex">(299 x 299)</script> image.</p>

<h4 id="transferability">Transferability</h4>

<p>Since iterative attacks optimize against a single network, the generated adversarial examples often do not work well against other networks. 
Different network architectures trained with gradient descent yield different local minima in parameter space, and an optimized adversarial example will not generalize well to a different architecture. 
It seems that adversarial examples can overfit just like networks can.</p>

<p>One-step methods often <a href="https://arxiv.org/abs/1705.07204">generalize better</a> than iterative methods, which leads to a single set of adversarial examples being a vulnerability for any network trained on the same dataset. 
This is far less effort for bad actors than optimizing against all networks and guessing which ones are deployed in the field.</p>

<h4 id="one-step-methods-are-a-simple-viable-threat-vector-against-deployed-ml-systems">One-step methods are a simple, viable threat vector against deployed ML systems</h4>

<p>If you deploy machine learning systems at any scale, you should consider the threat of adversarial inputs. 
If you have a highly tuned model architecture that you’re sure nobody else can deduce, then you might be safe from <em>some</em> well-optimized adversarial attacks. However one-step examples are cheap to create and can generalize across architectures.</p>


       </p>

    </section>


        </div><!-- /.inner-container -->
      </div>  <!-- /.outer-container -->
        </footer>

              </body>

</html>
