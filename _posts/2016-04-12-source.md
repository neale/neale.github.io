---
layout: post
title:  "Generating code with character level RNNs "
date:   2016-01-12 23:25:05
categories: NN
image: true
---



<!--To start off this **Blog** I thought I would give a history of where Ive been on my short ourney so for, starting with my first encounter with a neural net. A year or so ago I came across an npm package called Synaptic which gives some functions for defining the behavior of artificial neurons.-->

Well I said I was going to source code with my character RNN and last weekend I did just that. I grabbed the largest open source programs I could find. 
Obviously I took the linux kernel, and then I thought it would be interesting to see what a neural network thought of itself. So I concatonated the Torch source 
that's on Github and sent it through the LSTM. The last thing I did was take all of my code, every last piece of it that's on Github and tried to see what an RNN 
thinks of my code. Mostly I was sure that the Linux and Torch sources were going to give much cleaner representations of code than my own source will. 

## Linux Source Code

Without further delay, I downloaded the Linux source from [Github](https://github.com/torvalds/linux) as a frustratingly large repository of 1.5GB

In order to cat all the files into one disgustingly long file I ran `find linux/ -name '*' -exec cat {} \; > linux.txt`

This file was a lot larger than the C.S. Lewis corpus, and needed a correspondingly larger amount to train, but here is some of the sampled code. 
This time I'm going to cherry pick the results because I think that's more interesting. We already know these models aren't perfect. 


```c
-
-
-
```

> Should I even turn on highlighting for this. Linus is probably throwing a fit about deflowering C or something. 

### So I don't know what happened to this page, but I'm retraining for all these examples right now. Just sit tight for a couple days

## Torch Framework Generation

I was really excited for this, its a smaller dataset than the Linux source. I'm really considering adding in more Torch code from other repositories just to boster the dataset. 
But I can't control how valid someone else's Torch code is so that seems more likely to contaminate the dataset then help it. I still might do it. 

The methodology here is much the same, I downloaded the source from [Github](https://github.com/torch/torch7) and catted the whole thing with the above command.
The resulting text file was ~2MB so I figured it was good to go. Here's some of the generated source, this is how a neural net would build itself. 

```lua 

function LSTM:__init(value)
   end
   assert(step >= 1, "set '] then
      self:recursiveCopy(self.outputSize, t1)
   end

   -- get the it  convertis sequencen the input to return and (targets through forward for layers
   
   self._sequence[self.step-1] = torch.dim() do
      -- Uoth initialModule
      self.step = 1
      assert(torch.type(checkSums.localSumh == next_c)) 
      self._gradOutput = self.initialModule or torch.LongStorage(self.inputSize, output:size(2))
      if table.insert(self.modules[1] rho <= 1) and nStep >= 'both' then
         self.bias:=4:clone(), self.bwdSeq)
      return self
end

```

> It even got the commments right, but they're a little funky since the network doesn't have any language reference, just source code. 
> So all it knows how to talk about is networks, which probably works. 

> I was particularly impressed by this bit, it even got the dot operator right, ans it closes parentheses well.

```lua

assert(torch.type(checkSums.localSumh == next_c)) 
self._gradOutput = self.initialModule or torch.LongStorage(self.inputSize, output:size(2))
if table.insert(self.modules[1] rho <= 1) and nStep >= 'both' then

```

> This is totally reasonbable lua code that only doesn't work if you look really close. This really speaks to the quality of code that makes up Torch.
> The generated text would be way more convoluted if the source wasn't consistant. 

```lua

function matInput:clone()
   self.zeroTensor = self.recurrentModule
   self.output = gradOutput:clone()
   self.gradOutput = nil
   for step=nStep,1,10 do
      seq2.input = self.step * 1
      self._gradInputs = outputs[1]:sub() * sequence:size(1) 
      self.step = nil
      output:transpose(1,1):seq(self.outputsize)
      self.output:size(1) = self.gradOutput:clone()
end

```

Next I just need to train the RNN on my own code, this is going to be the most humbling experience of my programming life. 


