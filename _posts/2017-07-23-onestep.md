---
layout: post
title:  "Removing One-Step Adversarial Perturbations with Bilateral Filtering"
date:   2017-07-23 23:25:05
categories: NN 
image: true
mathjax: true
---


I don't think its escaped anyone that the state of the art of adversarial attacks completely outstrips the state of the art for defenses. 
All defenses including defensive distillation, logit classification a la. SafetyNet, and other statistical methods are currently broken. 
Mostly, these defenses fall to optimizaiton based adversarial attacks, because the current way we do things in machine learning and computer vision is invariably to train a classifier. 

Defensive distillation was a good idea: the authors basically fuzz the gradients of the network by training a second smaller network on the softmax outputs of the original network. 

So that a training sample of the second network has a distribution of labels assigned as the label, instead of a single hard label. This increases the number of features that need to be changed by an adversarial attack, in order to change the output label. That's a high level overview, I recommend the actual paper [here](https://arxiv.org/pdf/1511.04508.pdf)


This worked pretty well at first, until it was broken by [Carlini and Wagner](https://arxiv.org/pdf/1607.04311.pdf) with an iterative method that targets logits instead of the softmax output. 
Distillation relies heavily on the temperature of the primary network in order to train the distillation net. 
Which controls the distribution of soft labels that each example is assigned. 
Recap: a softmax output creates a $$[0, 1]$$ normalized distribution from the incoming logits. 

$$ F(y = softmax(Z(\theta, x) / T)$$

$$ softmax() = \frac{e^{zj}}{\sum_{k} e^{z_k}} $$

Where the temperature $$T$$ controls the shape of the output distribtion. When $$T \rightarrow \infty$$ the output becomes uniform, when $$T \rightarrow 0$$ the distribution become unimodal. 
