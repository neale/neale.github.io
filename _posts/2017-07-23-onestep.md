---
layout: post
title:  "Removing One-Step Adversarial Perturbations with Bilateral Filtering"
date:   2017-07-23 23:25:05
categories: NN 
image: true
mathjax: true
---


I don't think its escaped anyone that the state of the art of adversarial attacks completely outstrips the state of the art for defenses. 
All defenses including defensive distillation, logit classification a la. SafetyNet, and other statistical methods are currently broken. 
Mostly, these defenses fall to optimization based adversarial attacks, because the current way we do things in machine learning and computer vision is invariably to train a classifier.

$$ \textbf{y} = \mathit{overfit}(\textbf{x}) $$

In the context of adversarial attacks, training regimes invaribly create fully exploitable models. 
The CW attacks are all optimization based attacks, which are very much standard now. Optimization based attacks such as CW, the L-BFGS attack, and the Jacobian Based Saliency Map attack are far more effective at creating adversarial attacks than so called one-step attacks.
Most optimization attacks work by taking some image $$x$$ with some label $$y$$, and repeatly minimizing some objective s.t. some norm to get a similar image $$x'$$ with a different label $$y'$$ 

This is in contrast to one-step methods, which constructs the adversarial perturbation in a single go. 
The common example is the Fast Gradient Sign Method, which adds a perturbation $$\eta$$ in the direction of the incorrect class. 

$$ \eta = \textit{sign}(\nabla_x H(x, y')) $$

Where $$\textit{H(x, y')}$$ is the cross entropy loss w.r.t an image and an incorrect label.
Adversarial examples can be created quickly using only a single gradient computation. 

Compare this to some of the different objective functions of iterative methods, using the same notation as above. $$Z$$ here refers to the network logits, i.e. the input of the softmax layer, 

##### L-BFGS 

$$ \min c || x - x' ||^{2}_{2} + H(x') $$

##### CW ($$L_2$$)

$$ \min ||\eta - x||^2_2 + c f(\eta) $$

$$ \textit{where} \quad f(x') = \max(Z(x')_i - Z(x')_{y'}) \quad 
\textit{such that} \quad i \neq y'$$

I've simplified the above equations; removing the box constraints and the CW change of variables for readability. 
But the point is that we can choose a metric, and can optimize against our network to create effective adversarial examples with minimal distortion. 


{% if page.image %}
<div class="post-img">
<p align="center">
<img class="img-responsive img-post" src=" {{site.baseurl}}/assets/images/snake-adv-comp.svg" align="middle" width="90%" height="90%" />
</p>
</div>
{% endif %}

Given the above difference, the rationale behind one-step attacks seems suspect. Why bother with them at all if optimization based methods are so superior that the magnitude of the (normalized) perturbation is less than 1 across a $$(299 x 299)$$ image.

#### Transferability

Since iterative attacks optimize against a single network, the generated adversarial examples often do not work well against other networks. 
Different network architectures trained with gradient descent yield different local minima in parameter space, and an optimized adversarial example will not generalize well to a different architecture. 
It seems that adversarial examples can overfit just like networks can. 

One-step methods often [generalize better](https://arxiv.org/abs/1705.07204) than iterative methods, which leads to a single set of adversarial examples being a vulnerability for any network trained on the same dataset. 
This is far less effort for bad actors than optimizing against all networks and guessing which ones are deployed in the field. 

#### One-step methods are a simple, viable threat vector against deployed ML systems

If you deploy machine learning systems at any scale, you should consider the threat of adversarial inputs. 
If you have a highly tuned model architecture that you're sure nobody else can deduce, then you might be safe from _some_ well-optimized adversarial attacks. However one-step examples are cheap to create and can generalize across architectures. 

