---
layout: post
title:  "Narnia with word level Recurrant Neural Networks"
date:   2015-01-12 23:25:05
categories: NN 
image: true
---



<!-- neurons -->


Playing with Recurrant Neural Networks is one of those hobbies that really reminds me how powerful these models can be. 
Every since I read Andrej Karpathy's blog [post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) on the absurd power of RNNs
I wanted to try them on my own data. RNNs look more indimidating than other neural network architectures and they unravel data in time. 
That seems scary and difficult, I remember how hard it was to tune my first neural network and have been avoiding RNNs for a long time. 
But their potential to do amazing tasks is enticing. Deep learning can be hard to get people interested in, who might not otherwise have the inclination.
I want to show someone a demo that's better than drawing a better linear seperator than an SVM, or showing object recognition on an image. I want an RNN.
RNNs can learn to caption videos, generate music, or learn to speak. They can also be used to generate images in a crazy hallucination that only a neural net 
could come up with. 


## The Inside of an RNN

Recurrant Neural Networks look more like the goal of AI than a standard neural net. Vanilla feed forward neural networks can take an input, apply a learned function, 
and produce a fixed output. That is, regular neural networks can look at data and reason about it, but really only in some constrained way that's limited by their training scheme. 
The data has no spatial or temporal correlation and the network blindly produces an output. Convolutional Neural Networks were an effort to fix the lack of spatial reasoning 
with respect to images and neural networks. RNNs then are a way to encode information in time into a neural network. The goal of an RNN is to take sequences of data and model
the data as a distribution with respect to time. This puts RNNs in a whole different class of network called a generative model. Generative models can create new information, 
while vanilla nets only exist to answer a heavily formulated and constrained question.


{% if page.image %}
<div class="post-img">
<p align="center">
<img class="img-responsive img-post" src=" {{site.baseurl}}/img/RNN_basic.png " align="middle" width="40%" height="40%" />
</p>
</div>
{% endif %}

> Here each circle here is the usual neuron activation. But the main interest is that there are arrows connecting previous layers in the sequence, to layers that come later.
> Each wtx+b computation is a vector of fixed size and is preformed over t timesteps (where t is the number of layers in the RNN) for each neuron. Given this history, the 
> RNN can not only reason about its current input like a regular feed forward neural net does, but it can look back t timesteps at the data that preceded the current input
> and use that data to more effectively train the generator.

It gets better, RNNs are so powerful that they can operate on any data and draw out a better representation than other machine learning methods because they can develop a 
broader view, or rather contextualize their input. Really by feeding sequences into RNNs, you're giving a neural network different states of some larger program, 
and the job of the RNN is to generate a new state.  If you were to give an image to an RNN and a sequence of pixels, then the RNN could generate new states for the image 
for every time step. If RNNs take an input, and produce a new state, than they are in fact generating programs. RNNs are [turing complete](https://arxiv.org/pdf/1410.5401.pdf) 
and can be generalized to form Neural Turing Machines, which build internal states, given an input and output. In other words, given an input and the transformed output, 
a Neural Turing Machine (RNN) can develop the black boxed algorithm that transformes the input state to the output state. 


