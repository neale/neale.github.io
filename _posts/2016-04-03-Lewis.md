---
layout: post
title:  "Narnia With Character Level Recurrant Neural Networks"
date:   2015-01-12 23:25:05
categories: NN 
image: true
---



<!-- neurons -->


Playing with Recurrant Neural Networks is one of those hobbies that really reminds me how powerful these models can be. 
Every since I read Andrej Karpathy's blog [post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) on the absurd power of RNNs
I wanted to try them on my own data. RNNs look more indimidating than other neural network architectures and they unravel data in time. 
That seems scary and difficult, I remember how hard it was to tune my first neural network and have been avoiding RNNs for a long time. 
But their potential to do amazing tasks is enticing. Deep learning can be hard to get people interested in, who might not otherwise have the inclination.
I want to show someone a demo that's better than drawing a better linear seperator than an SVM, or showing object recognition on an image. I want an RNN.
RNNs can learn to caption videos, generate music, or learn to speak. They can also be used to generate images in a crazy hallucination that only a neural net 
could come up with. 


## The Inside of an RNN

Recurrant Neural Networks look more like the goal of AI than a standard neural net. Vanilla feed forward neural networks can take an input, apply a learned function, 
and produce a fixed output. That is, regular neural networks can look at data and reason about it, but really only in some constrained way that's limited by their training scheme. 
The data has no spatial or temporal correlation and the network blindly produces an output. Convolutional Neural Networks were an effort to fix the lack of spatial reasoning 
with respect to images and neural networks. RNNs then are a way to encode information in time into a neural network. The goal of an RNN is to take sequences of data and model
the data as a distribution with respect to time. This puts RNNs in a whole different class of network called a generative model. Generative models can create new information, 
while vanilla nets only exist to answer a heavily formulated and constrained question.


{% if page.image %}
<div class="post-img">
<p align="center">
<img class="img-responsive img-post" src=" {{site.baseurl}}/img/RNN_basic.png " align="middle" width="40%" height="40%" />
</p>
</div>
{% endif %}

> Here each unit is an arbitrary basis function, it can be a ReLU, Tanh, or anything else. But the main interest is that there are arrows connecting previous layers in the sequence, to layers that come later.
> Each wtx+b computation is a vector of fixed size and is preformed over t timesteps (where t is the number of layers in the RNN) for each neuron. Given this history, the 
> RNN can not only reason about its current input like a regular feed forward neural net does, but it can look back t timesteps at the data that preceded the current input
> and use that data to more effectively train the generator.

It gets better, RNNs are so powerful that they can operate on any data and draw out a better representation than other machine learning methods. They can develop a 
broader view, or rather contextualize their input by remembering past states of data. Really by feeding sequences into RNNs, you're giving a neural network different states of some larger program, 
and the job of the RNN is to generate a new state.  If you were to give an image to an RNN as a sequence of pixels, then the RNN could generate new states for the image 
for every time step. If RNNs take an input, and produce a new state, than they are in fact generating programs. RNNs are [turing complete](https://arxiv.org/pdf/1410.5401.pdf) 
and can be generalized to form Neural Turing Machines, which build internal states, given an input and output. In other words, given an input and the transformed output, 
a Neural Turing Machine (RNN) can develop the black boxed algorithm that transformes the input state to the output state. Here's a look at what a Neural Turing Machine looks like. 



{% if page.image %}
<div class="post-img">
<p align="center">
<img class="img-responsive img-post" src=" {{site.baseurl}}/img/NTM.png " align="middle" width="40%" height="40%" />
</p>
</div>
{% endif %}

> wat


With all this in mind. There's a great open source community based around getting these models up and running in real life. Turnaround from research paper to open source implementation 
is pretty quick these days. As you know, I prefer Torch for pretty much everything. Now Torch isn't great for RNNs, but I don't want to use Theano and Lua is pretty friendy for hacking 
together computational kernels in Torch. I have a lot of things I want to make RNNs do, but with the release of a [convnet approach to copying painting styles](http://arxiv.org/abs/1508.06576) 
I thought it would be really interesting to teach an RNN to speak english in the style of one of my favorite authors. My desktop is busted so I'll be training it on an M5 mobile 
processor, but it should work anyway. RNNs don't need to be as deep as the more interesting CNN architectures (I'm looking at you ResNet), I'd never finish anything 
with a 200 layer RNN. So that's the task, in Torch we want to create an RNN that can learn to speak like C.S. Lewis. 

## LSTMs

So lets start building a network. We're going to work with the standard assumption that deep = good. We're going to use torch to stack layers of an RNN in order learn a 
complicated representation of our input-output mapping, or just call it function approximation. And going to train with backprop to optimize a convex error function. 
At the end were going to generate probabilities and pick the likely candidate from the distribution. This all sound standard and boring? Great, now lets start getting weird. 
We're not going to use a Vanilla RNN, because in practice they don't preform as well as other architectures. The most common flavor of RNN that's used in practice is called an LSTM, 
for long short term memory. Its named so because the nodes in an LSTM aren't just mappings to an activation, but instead are their own network of gates and cells. This allows a 
greater measure of control over the memory of the RNN. 


{% if page.image %}
<div class="post-img">
<p align="center">
<img class="img-responsive img-post" src=" {{site.baseurl}}/img/LSTM.png " align="middle" width="40%" height="40%" />
</p>
</div>
{% endif %}

> These diagrams are notoriously unintuitive. But the basic idea is that for each of these cells, there is an associated parameter in the network that is learned. 
> The network will learn when to store more information, when to forget (zero the cell), and what operations to preform on the data before the output. 

> The only other thing to note is the sigmoid units before each dot product, this just prevents the gradients from blowing up, which is a common problem with RNNs. 
> I should actually just talk about it.

## Gradient Explosion and Vanishing
RNNs are not a new concept, they were first formulated back in the 1980s, with portions of it drawn from older boltzman machines, and others from the even older STM equation 
which describes the application of memory cells but none of that really matters because RNNs are here and they work. But they didn't always, RNN's were rarely used up until a few 
years ago because they were so difficult to train. As with regular neural nets, if you can stabilize the gradients and keep them from saturating, then you can effectively use backprop to 
find a minima of your manifold and train your network. The problem was that this just wasn't happening with recurrent nets, and the reason was because of backpropogation. 

 
{% if page.image %}
<div class="post-img">
<p align="center">
<img class="img-responsive img-post" src=" {{site.baseurl}}/img/backpass.png.png " align="middle" width="40%" height="40%" />
</p>
</div>
{% endif %}

> The backward pass through a network, you will accumulate all the gradients dz, and sum them for each layer. 

{% if page.image %}
<div class="post-img">
<p align="center">
<img class="img-responsive img-post" src=" {{site.baseurl}}/img/backprop_eq.gif " align="middle" width="40%" height="40%" />
</p>
</div>
{% endif %}

> The chain rule representation of the backward pass of backpropogation

During backpropogation you're accumulating all these gradients at each layer, and you are applying them to a basis function like a ReLU and praying 
that your neurons don't die. If you do this accumulation with a simple feed forward model, then you just have summed gradients i.e. standard backprop, and there is no issue
 as long as you regularize and use an intelligent basis function. The difference in recurrent nets is that you are multiplying derivitives through multiple time steps 
at each layer. So you can think about it in this exponential framework where if you have a recurrent net and you're looking back 100 time steps, then you're going to be 
multiplying 100 derivitives before you can pass the input to a squashing unit. If your gradients are greater than 1, the signal explodes to infinity, if they're less than 1, 
then the gradient goes to 0 and the cell dies. This is why people had so much trouble training RNNs with regular backprop. The signals weren't propogating useful 
information and the memory structure wasn't useful in practice. 

This is why people started using gating, so the gradient signal could be effectively controlled as the network learned more and more parameters further back in time. 
This is an LSTM cell in Torch, which is far simpler than the diagram makes it look. 

We'll be defining a simple LSTM cell that ca be stacked as a unit, in layers that are n cells wide 


```lua

-- define a computation graph with nngraph  
local input = {}

local in     = input[1]
local prev_c = input[2]
local prev_h = input[3]

table.insert(input, nn.Identity()())  
table.insert(input, nn.Identity()())  
table.insert(input, nn.Identity()())  

-- define layer-wise connections

local input_to_hidden  = nn.Linear(input_size, 4 * rnn_size)(in)
local hidden_to_hidden = nn.Linear(rnn_size, 4 * rnn_size)(prev_h)
local signals          = nn.CAddTable()({input_to_hidden, hidden_to_hidden})

```

> We just defined the input table, which will store our input data, and the connections that regulate dataflow in the graph.
> Remember that rnn_size is the width of our network and is used to define how layers are connected. 

> From tables, we're going to be interting tensors right into the LSTM input, and defining the gates with activations

```lua
-- input gate
local in_signal = nn.Narrow(2, rnn_size+1, 2*rnn_size)(signals)
local in_gate   = nn.Tanh()(in_signal)
-- other gates
local signal = nn.Narrow(2, 1, 2 * rnn_size)(signals)
local squash = nn.Tanh()(signal)
```

> Now for the gate definitions themselves.

```lua
local input_g  = nn.Narrow(2, 1, rnn_size)(squash)
local forget_g = nn.Narrow(2, rnn_size+1, rnn_size)(squash)
local output_g = nn.Narrow(2, 2*rnn_size+1, rnn_size)(squash)
```

The gradients have been secured, and the gates have been defined. We need to add the computations to the table so that we can make them apart of the pipeline.
It will just take another call to `nn.CAddTable` and `nn.CMulTable`

```lua
local c_input  = nn.CMulTable()({input_g, prev_c}) -- input cell gate state
local c_forget = nn.CMulTable()({input_g, in_gate}) -- forget cell gate state
local next_c   = nn.CAddTable()({
    c_input, 
    c_forget
})
```

> Next just get the output of the LSTM cell by multiplying by tanh and the output table

```lua
local c_out  = nn.Tanh()(next_c)
local next_h = nn.CMulTable()({output_g, c_out})
```

And that's it. We have defined an LSTM cell that we can replicate in as many layers as we want and train it with backpropogation. 

## Training on Narnia

It took a long time but I managed to find and concatonate the entire published works of C.S. Lewis into a single file and loaded it into the LSTM one character at a time. 
The whole file was just shy of 3MB, too small to be very useful, but I figured I'd learn something interesting. So I trained it for a day or so on my M5 laptop and here's 
an excerpt when I sampled it. 

> One stormy night perfectly swelling disappeared, and then they found he himself felt a very touch for his ear. And then she had got the person on both chest he remoined in a red
> green standing up to his head broke in the air of that. Then he was blowing noises. What
> he was Polly and great difficulty: and then he appeared to be quite as explorable faming like frove more changed.
> He began looked during his morning in the sea, there was some of them all had been beyond that when he was saying so hnau. They went altogether in his eyes when he had carrier an
> with in my opening powerfalls in the thing in the first bird in the warm mind, bowed and sometimes made them all open to the beautiful feet of Malacandra. The Man lived as he cam
> call a tree in the end. And the reflecting particial bright conversation.

> In a tree so fine, but the words he would have taken it on the very finest, but for him whole world and be knowed by old Tisroc's name. 
> But the slipper of stopping it, rabbit home he the secret glorious lion had afterway and has forced upon theology. No bigger phase it must still be true-if.
> What shall - "How, go on."
> "Hear of my arm moved," began Devine.
> "I'm glad so you first," said Ransom.
> But which was hoping. Their first noise under has readdrist with a marble and threw. Or courage for he worked their trees.
> He gave grey unpretent to say they had all already thrown it on one anoposity of very free-less wonderfully. To say which please of course he have had as well as the goldey of wings and some superour whose not a
> drink of thinking, "If they'd be ade enforalisorshight," whispered Christianity-and the garden (not to be brother,
> and so this would be surprised
> Now discovery than this becton parties will give to get round to wider evil also you lost it because God
> and forced  it, the lighting of a nice bringing images to be cried. You shape offered by an offing to God. You mistake God is it now?  
> Now had gone about you are going on, sorry man

I didn't cherry pick those examples or clean up the outputs. You can see some inconsistances in its learning. Clearly it hadn't quite converged, and the training 
data wasn't extensive enough. Obviously we have some creativity with Ransom and Devine having a conversation, as they would in the space trilogy. 

> "Hear of my arm moved," began Devine.

> "I'm glad so you first," said Ransom.

Even though there is no central context happening here, you can see that from characters, the RNN has learned about grammer rules. 

* Spaces after periods. 

* Commas between thoughts. 

* How to open and close quotation marks

* Starting sentences with capital letters


The RNN has a mastery of English, even if it doesn't have any coherant thoughts. I hope this has inspired you to build an RNN of your own and make robots generate cool 
things. I want to try music and source code next, and I really want to check out neural turing machines more. 


