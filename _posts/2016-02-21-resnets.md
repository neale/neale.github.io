---
layout: post
title:  "Ultra Deep Networks"
date:   2016-07-12 23:25:05
categories: NN
image: true
---
<!-- buffer -->

# All About ResNets

I've been wanting to write a post about resnets for some time. They seem like the coolest thing to come out in the last year (I'm looking at you deep compression). 
Resnets are the brain child of Microsoft Research Asia, and they swpet not just ILSVRC 2015, but also MS COCO, Both of which are difficult datasets with lots of competition. 
The most interesting part about resnets is that they were implemented as CNNs with huge depth. Where before a very deep network would look like Google's Inception or VGGnet with 15-20+
layers before threshholding accuracy. Resnets were implemented successfully with 152 layer nets, I encourage you to [read the paper](https://arxiv.org/abs/1512.03385) for a primer. 
Microsoft achieved a 6.7% top 5 error rate on Imagenet, which should sit right around human performance. These results were passed recently by facebook's torch implementation where
they took the model to 200 layers and still saw an increase in accuracy beyond MSRA. 

## Facebook's ResNet

Facebook recently implemented ResNet and improved on the results presented in the original paper. They open sourced their implementation and uploaded the weights for 
all of their models. Since training a ResNet with your own machine would take weeks, downloading the pretrained weights would be the way to go. So if all you want to do is 
plug the model into an existing framework, then head over to their [github repo](https://github.com/facebook/fb.resnet.torch)


{% if page.image %}
<div class="post-img">

<img class="img-responsive img-post" src=" {{site.baseurl}}/images/facebook_resnet_results.png " width="40%" height="40%" align="middle" />
</div>
{% endif %}


### A Short History of Neural Nets

Before going into the details of ResNet, lets look why we even want to make them deeper. 
When the first neural network's began as perceptrons back in the 60s. They largely consisted of an input layer, an output layer, and a single hidden layer. 
The input function (WtX + b) was applied to an activation (usually a heaviside or logit function), and the parameters were updated via gradient descent. 
This framed learning as a convex optimization problem and allowed the perceptron to approximate complex functions by finding the bottoms of the error hyperparaboloid. 
This is great, except that they didn't work very well, the gradients got all mashed up and either exploded to infinity, or were killed.  So time passed 
and Geoffery Hinton figured out weight initializations, and the community got used to using backpropogation to train these models.  When CNNs became popular in [2012](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-),
The idea started that deeper nets are able to reason about their input at a higher level than shallow nets. If you think of CNNs as machines that build a hierarchy of 
feature maps with each hidden layer, then heuristically you can reason that a deeper net can learn more abstract features. 

> Note : This isn't strictly true, a shallow net can learn any function as well as a deep net, but deep nets can be better optimized with fewer parameters. 
> Microsoft has a good [paper](https://arxiv.org/pdf/1312.6184.pdf) on how shallow nets are just as good if not better than deep nets. 


{% if page.image %}
<div class="post-img">
<p align="center">
<img class="img-responsive img-post" src=" {{site.baseurl}}/public/images/MLP.jpg " align="middle" width="40%" height="40%" />
</p>
</div>
{% endif %}

{% if page.image %}
<div class="post-img">
<p align="center">
<img class="img-responsive img-post" src=" {{site.baseurl}}/public/images/DNN.png " width="60%" height="60%" />
</p>
</div>
{% endif %}


